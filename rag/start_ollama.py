#!/usr/bin/env python3
"""
Ollama Setup Helper
==================

This script helps set up Ollama and the lawgorithm model for the RAG system.
"""

import subprocess
import sys
import time
import requests
import os

def check_ollama_installed():
    """Check if Ollama is installed"""
    try:
        result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"‚úÖ Ollama is installed: {result.stdout.strip()}")
            return True
        else:
            print("‚ùå Ollama is not properly installed")
            return False
    except FileNotFoundError:
        print("‚ùå Ollama is not installed")
        print("üí° Install Ollama from: https://ollama.ai/")
        return False

def start_ollama_server():
    """Start Ollama server"""
    print("üöÄ Starting Ollama server...")
    try:
        # Check if Ollama is already running
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Ollama server is already running!")
            return True
    except:
        pass
    
    try:
        # Start Ollama server in background
        if os.name == 'nt':  # Windows
            subprocess.Popen(['ollama', 'serve'], 
                           stdout=subprocess.DEVNULL, 
                           stderr=subprocess.DEVNULL)
        else:  # Unix/Linux/Mac
            subprocess.Popen(['ollama', 'serve'], 
                           stdout=subprocess.DEVNULL, 
                           stderr=subprocess.DEVNULL,
                           start_new_session=True)
        
        # Wait for server to start
        print("‚è≥ Waiting for Ollama server to start...")
        for i in range(30):  # Wait up to 30 seconds
            try:
                response = requests.get("http://localhost:11434/api/tags", timeout=5)
                if response.status_code == 200:
                    print("‚úÖ Ollama server started successfully!")
                    return True
            except:
                pass
            time.sleep(1)
        
        print("‚ùå Ollama server failed to start within 30 seconds")
        return False
        
    except Exception as e:
        print(f"‚ùå Error starting Ollama server: {e}")
        return False

def check_lawgorithm_model():
    """Check if lawgorithm model is available"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=10)
        if response.status_code == 200:
            models = response.json().get('models', [])
            model_names = [model['name'] for model in models]
            
            if "lawgorithm:latest" in model_names:
                print("‚úÖ lawgorithm:latest model is available!")
                return True
            else:
                print("‚ùå lawgorithm:latest model not found")
                return False
    except Exception as e:
        print(f"‚ùå Error checking models: {e}")
        return False

def pull_lawgorithm_model():
    """Pull the lawgorithm model"""
    print("üì• Pulling lawgorithm:latest model...")
    print("‚ö†Ô∏è This may take several minutes depending on your internet connection...")
    
    try:
        result = subprocess.run(['ollama', 'pull', 'lawgorithm:latest'], 
                              capture_output=True, text=True)
        
        if result.returncode == 0:
            print("‚úÖ lawgorithm:latest model pulled successfully!")
            return True
        else:
            print(f"‚ùå Error pulling model: {result.stderr}")
            return False
    except Exception as e:
        print(f"‚ùå Error pulling model: {e}")
        return False

def main():
    """Main setup function"""
    print("üîß Ollama Setup Helper")
    print("=" * 30)
    
    # Check if Ollama is installed
    if not check_ollama_installed():
        print("\nüí° Please install Ollama first:")
        print("   Windows: Download from https://ollama.ai/")
        print("   Mac: brew install ollama")
        print("   Linux: curl -fsSL https://ollama.ai/install.sh | sh")
        return
    
    # Start Ollama server
    if not start_ollama_server():
        print("\n‚ùå Failed to start Ollama server")
        print("üí° Try running 'ollama serve' manually in a terminal")
        return
    
    # Check if lawgorithm model is available
    if not check_lawgorithm_model():
        print("\nüì• lawgorithm model not found, pulling...")
        if not pull_lawgorithm_model():
            print("\n‚ùå Failed to pull lawgorithm model")
            print("üí° Try running 'ollama pull lawgorithm:latest' manually")
            return
    
    print("\nüéâ Setup completed successfully!")
    print("üí° You can now run the conversational RAG system:")
    print("   python rag/conversational_lawgorithm_rag.py")

if __name__ == "__main__":
    main() 